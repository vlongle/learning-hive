{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from shell.utils.experiment_utils import *\n",
    "from shell.utils.metric import *\n",
    "import matplotlib.pyplot as plt\n",
    "from shell.fleet.network import TopologyGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root_dir = \"debug_replay_results\"\n",
    "dataset = \"mnist\"\n",
    "# algo = \"monolithic\"\n",
    "algo = \"modular\"\n",
    "num_train = 64\n",
    "seed = 0\n",
    "use_contrastive = True\n",
    "# use_contrastive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root_dir = \"debug_replay_results\"\n",
    "dataset = \"cifar100\"\n",
    "# algo = \"monolithic\"\n",
    "algo = \"modular\"\n",
    "num_train = 256\n",
    "seed = 0\n",
    "use_contrastive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"{dataset}_{algo}_numtrain_{num_train}\"\n",
    "if use_contrastive:\n",
    "    job_name += \"_contrastive\"\n",
    "experiment = os.path.join(save_root_dir, job_name, dataset,algo, f\"seed_{seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'component_update_freq': 2, 'num_epochs': 2, 'init_component_update_freq': 2, 'init_num_epochs': 2, 'save_freq': 20}, 'dataset': {'dataset_name': 'cifar100', 'num_tasks': 5, 'num_classes_per_task': 5, 'with_replacement': False, 'num_trains_per_class': 256, 'num_vals_per_class': -1, 'remap_labels': True}, 'net': {'name': 'cnn', 'depth': 4, 'channels': 50, 'conv_kernel': 3, 'maxpool_kernel': 2, 'padding': 1, 'dropout': 0.5}, 'sharing_strategy': {'name': 'no_sharing', 'num_coms_per_round': 0}, 'seed': 0, 'algo': 'modular', 'job_name': 'cifar100_modular_numtrain_256', 'num_agents': 2, 'root_save_dir': 'debug_replay_results', 'parallel': True, 'num_init_tasks': 4, 'agent': {'save_dir': '${root_save_dir}/${job_name}/${dataset.dataset_name}/${algo}/seed_${seed}', 'batch_size': 64, 'memory_size': 32, 'improvement_threshold': 0.05, 'use_contrastive': False}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = os.path.join(experiment, \"hydra_out\", \".hydra\", \"config.yaml\")\n",
    "# read the config file\n",
    "cfg = omegaconf.OmegaConf.load(config_path)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'component_update_freq': 2, 'num_epochs': 2, 'init_component_update_freq': 2, 'init_num_epochs': 2, 'save_freq': 20}, 'dataset': {'dataset_name': 'cifar100', 'num_tasks': 5, 'num_classes_per_task': 5, 'with_replacement': False, 'num_trains_per_class': 256, 'num_vals_per_class': -1, 'remap_labels': True}, 'net': {'name': 'cnn', 'depth': 4, 'channels': 50, 'conv_kernel': 3, 'maxpool_kernel': 2, 'padding': 1, 'dropout': 0.5}, 'sharing_strategy': {'name': 'no_sharing', 'num_coms_per_round': 0}, 'seed': 0, 'algo': 'modular', 'job_name': 'cifar100_modular_numtrain_256', 'num_agents': 2, 'root_save_dir': 'debug_replay_results', 'parallel': True, 'num_init_tasks': 4, 'agent': {'save_dir': '${root_save_dir}/${job_name}/${dataset.dataset_name}/${algo}/seed_${seed}', 'batch_size': 64, 'memory_size': 32, 'improvement_threshold': 0.05, 'use_contrastive': False}}\n",
      "i_size 32\n",
      "num_classes 5\n",
      "net_cfg {'name': 'cnn', 'depth': 4, 'channels': 50, 'conv_kernel': 3, 'maxpool_kernel': 2, 'padding': 1, 'dropout': 0.5, 'i_size': 32, 'num_classes': 5, 'num_tasks': 5, 'num_init_tasks': 4}\n",
      "<class 'shell.learners.er_dynamic.CompositionalDynamicER'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph, datasets, NetCls, LearnerCls, net_cfg, agent_cfg, train_cfg = setup_experiment(cfg)\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([97, 66, 33, 19, 34,  3, 12,  2, 13, 92, 80,  5, 27, 60, 36, 11,  9,\n",
       "        25, 31, 87, 58, 44, 94, 95, 64, 61, 62, 86, 79, 98, 93, 51, 22, 59,\n",
       "        26, 37, 83, 73, 78,  1, 30, 55, 85, 47, 41, 20, 63, 43, 90, 88, 21,\n",
       "        40, 15, 75,  4,  0, 52, 23, 35, 53, 84, 69, 16, 10, 71, 81,  6, 91,\n",
       "        56, 76, 38, 50, 57, 74, 49, 18, 24, 39, 48, 99,  8, 65, 89, 67, 54,\n",
       "        32, 42, 14, 68, 45, 96, 77, 46, 28, 72, 17, 29, 82,  7, 70]),\n",
       " array([13, 55, 29, 67, 64, 47, 58, 19, 30, 38, 15, 16, 34, 35, 73, 96, 25,\n",
       "        89, 63,  5, 44, 21, 28, 60, 80, 86, 36, 95, 83, 40,  2, 24, 37, 61,\n",
       "        62, 57, 53, 79, 27, 82, 65,  4, 22, 45, 78, 33, 32,  9, 90, 56, 46,\n",
       "        48,  7, 59, 41, 93, 98, 69, 51, 74, 68, 72, 71, 39, 54, 18, 91, 76,\n",
       "        70, 11, 10, 99, 77, 52, 87, 31,  6,  1, 84, 43, 85, 42, 12, 94, 26,\n",
       "        88, 17, 49, 75, 92,  8, 97, 66, 50,  0, 23, 20, 81, 14,  3])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_sequence_list = [dataset.class_sequence for dataset in datasets]\n",
    "classes_sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNSoftLLDynamic(\n",
       "  (structure): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 4x4 (GPU 0)]\n",
       "      (1): Parameter containing: [torch.float32 of size 4x4 (GPU 0)]\n",
       "      (2): Parameter containing: [torch.float32 of size 4x4 (GPU 0)]\n",
       "      (3): Parameter containing: [torch.float32 of size 4x4 (GPU 0)]\n",
       "      (4): Parameter containing: [torch.float32 of size 4x4 (GPU 0)]\n",
       "  )\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (components): ModuleList(\n",
       "    (0-3): 4 x Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (decoder): ModuleList(\n",
       "    (0-4): 5 x Linear(in_features=200, out_features=5, bias=True)\n",
       "  )\n",
       "  (transform): Normalize(mean=(0.5079, 0.4872, 0.4415), std=(0.2676, 0.2567, 0.2765))\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=200, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 4\n",
    "agent_id = 0\n",
    "num_added_components = None\n",
    "net = load_net(cfg, NetCls, net_cfg, agent_id=agent_id, task_id=task_id, num_added_components=num_added_components)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <torch.utils.data.dataloader.DataLoader at 0x7f4436e3ab00>,\n",
       " 1: <torch.utils.data.dataloader.DataLoader at 0x7f4436e3b760>,\n",
       " 2: <torch.utils.data.dataloader.DataLoader at 0x7f4436e3b5e0>,\n",
       " 3: <torch.utils.data.dataloader.DataLoader at 0x7f4436e3bd00>,\n",
       " 4: <torch.utils.data.dataloader.DataLoader at 0x7f4436e3bbb0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets[agent_id]\n",
    "testloaders = {task: torch.utils.data.DataLoader(testset,\n",
    "                                                         batch_size=128,\n",
    "                                                         shuffle=False,\n",
    "                                                         num_workers=0,\n",
    "                                                         pin_memory=True,\n",
    "                                                         ) for task, testset in enumerate(dataset.testset[:(task_id+1)])}\n",
    "# testloaders = {task: torch.utils.data.DataLoader(testset,\n",
    "#                                                          batch_size=128,\n",
    "#                                                          shuffle=False,\n",
    "#                                                          num_workers=0,\n",
    "#                                                          pin_memory=True,\n",
    "#                                                          ) for task, testset in enumerate(dataset.valset[:(task_id+1)])}\n",
    "testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.308, 1: 0.482, 2: 0.474, 3: 0.332, 4: 0.21, 'avg': 0.3612}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = {}\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for task_id, testloader in testloaders.items():\n",
    "        correct = 0.\n",
    "        n = 0.\n",
    "        for X, y in testloader:\n",
    "            X = X.to(net.device)\n",
    "            y = y.to(net.device)\n",
    "            Y_hat = net(X, task_id)\n",
    "            correct += (Y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += len(y)\n",
    "            acc[task_id] = correct/n\n",
    "\n",
    "acc[\"avg\"] = sum(acc.values())/len(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.structure[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_out = [] # features\n",
    "y_out = [] # global labels\n",
    "y_task = [] # globallabel_task_id\n",
    "with torch.no_grad():\n",
    "    for task_id, testloader in testloaders.items():\n",
    "        for X, y in testloader:\n",
    "            X = X.to(net.device)\n",
    "            X_encode = net.contrastive_embedding(X, task_id)\n",
    "            X_out.append(X_encode.cpu())\n",
    "            y_out.append(y.cpu())\n",
    "            y_task.append(np.ones_like(y) * task_id)\n",
    "X_encode = np.concatenate(X_out, axis=0)\n",
    "Y = np.concatenate(y_out, axis=0)\n",
    "y_task = np.concatenate(y_task, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.class_sequence)\n",
    "print(np.unique(Y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, random_state=0, init=\"pca\", n_jobs=-1).fit_transform(X_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for y_label in np.unique(Y):\n",
    "    plt.scatter(X_embedded[Y == y_label, 0], X_embedded[Y == y_label, 1], label=y_label)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array same size as y and y_task where each element is {y}_{y_task} string\n",
    "y_task_str = np.array([str(Y[i]) + \"_\" + str(y_task[i]) for i in range(len(Y))]) # class_task\n",
    "# plot X_embedded with color corresponding to y_task_str\n",
    "import seaborn as sns\n",
    "# different sns color palette\n",
    "# bigger plot size\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "sns.set_palette(\"tab20\")\n",
    "\n",
    "task = 0\n",
    "X_t = X_embedded[y_task == task]\n",
    "y_t = y_task_str[y_task == task]\n",
    "sns.scatterplot(x=X_t[:, 0], y=X_t[:, 1], hue=y_t);\n",
    "# sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y_task_str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fc759119c7a949f50d7f999e302b90a14fbc886f397e56263de77303ed14594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
