{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from shell.fleet.helper import get_fleet, get_agent_cls\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from shell.utils.experiment_utils import setup_experiment, process_dataset_cfg, eval_net\n",
    "from shell.datasets.datasets import get_dataset\n",
    "from pprint import pprint\n",
    "import ray\n",
    "import os\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from shell.utils.utils import seed_everything\n",
    "from shell.fleet.model_sharing_utils import is_in, diff_models\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_552631/3645537571.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"conf\", job_name=\"tmp_job\")\n",
      "/home/vlongle/miniconda3/envs/shell/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'grad_mod': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "initialize(config_path=\"conf\", job_name=\"tmp_job\")\n",
    "cfg = compose(config_name=\"grad_mod\")\n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override\n",
    "cfg.parallel = True\n",
    "cfg.train.num_epochs = 50\n",
    "cfg.train.component_update_freq = 50\n",
    "\n",
    "\n",
    "# cfg.train.num_epochs = 1\n",
    "# cfg.train.component_update_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Class sequence: [4 7 0 8 4 6 4 3 8 0 1 0 5 9 6 4 9 0 3 0]\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'component_update_freq': 50, 'num_epochs': 50, 'save_freq': 1}, 'dataset': {'dataset_name': 'mnist', 'num_tasks': 10, 'num_classes_per_task': 2, 'with_replacement': True, 'num_trains_per_class': 64, 'num_vals_per_class': 50, 'remap_labels': True}, 'net': {'name': 'mlp', 'depth': 2, 'layer_size': 64, 'dropout': 0.0}, 'sharing_strategy': {'name': 'gradient', 'num_coms_per_round': 5, 'retrain': {'num_epochs': 10}}, 'seed': 0, 'algo': 'modular', 'job_name': 'fun', 'num_agents': 4, 'root_save_dir': 'results', 'parallel': True, 'num_init_tasks': 2, 'agent': {'save_dir': '${root_save_dir}/${job_name}/${dataset.dataset_name}/${algo}/seed_${seed}', 'batch_size': 64, 'memory_size': 64, 'improvement_threshold': 0.05, 'use_contrastive': True}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [2 1 3 8 6 0 3 0 5 0 3 6 2 7 6 7 6 1 0 5]\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [0 9 7 2 1 8 6 1 6 4 5 7 8 0 2 3 0 3 9 6]\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [0 5 3 4 4 5 9 3 0 6 9 1 3 1 7 9 5 2 3 2]\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_size 28\n",
      "num_classes 2\n",
      "net_cfg {'name': 'mlp', 'depth': 2, 'layer_size': 64, 'dropout': 0.0, 'i_size': 28, 'num_classes': 2, 'num_tasks': 10, 'num_init_tasks': 2}\n",
      "<class 'shell.learners.er_dynamic.CompositionalDynamicER'>\n"
     ]
    }
   ],
   "source": [
    "AgentCls = get_agent_cls(cfg.sharing_strategy, cfg.algo, cfg.parallel)\n",
    "\n",
    "graph, datasets, NetCls, LearnerCls, net_cfg, agent_cfg, train_cfg = setup_experiment(\n",
    "        cfg)\n",
    "FleetCls = get_fleet(cfg.sharing_strategy, cfg.parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Class sequence: [3 9 2 4 9 2 5 4 8 9 9 4 1 6 9 6 8 0 5 1]\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "INFO:root:(128, 1, 28, 28)\n",
      "2023-03-28 18:18:51,910\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:Agent: node_id: 69420, seed: 69420000\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.208\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.208\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.201\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.201\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:final components: 2\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 0, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.201\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.210\tacc: 0.577\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.206\tacc: 0.538\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 1, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.190\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.163\tacc: 0.537\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.177\tacc: 0.518\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 2, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.167\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.127\tacc: 0.538\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.147\tacc: 0.519\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 3, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.103\tacc: 0.577\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.130\tacc: 0.539\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 4, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.134\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.098\tacc: 0.646\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.116\tacc: 0.573\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 5, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.146\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.061\tacc: 0.747\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.104\tacc: 0.623\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 6, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.089\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.058\tacc: 0.833\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.074\tacc: 0.666\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 7, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.094\tacc: 0.500\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.021\tacc: 0.863\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.057\tacc: 0.682\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 8, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.119\tacc: 0.502\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.993\tacc: 0.883\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.056\tacc: 0.693\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 9, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.089\tacc: 0.518\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.986\tacc: 0.911\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.038\tacc: 0.714\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 10, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.126\tacc: 0.531\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.997\tacc: 0.927\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.062\tacc: 0.729\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 11, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.083\tacc: 0.530\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.985\tacc: 0.936\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.034\tacc: 0.733\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 12, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.029\tacc: 0.537\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 6.003\tacc: 0.943\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.016\tacc: 0.740\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 13, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.020\tacc: 0.574\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.924\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.972\tacc: 0.761\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 14, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.025\tacc: 0.643\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.966\tacc: 0.955\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.996\tacc: 0.799\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 15, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.056\tacc: 0.702\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.976\tacc: 0.954\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 6.016\tacc: 0.828\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 16, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.007\tacc: 0.746\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.908\tacc: 0.943\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.957\tacc: 0.845\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 17, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.017\tacc: 0.770\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.970\tacc: 0.925\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.994\tacc: 0.847\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 18, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.002\tacc: 0.807\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.918\tacc: 0.928\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.960\tacc: 0.867\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 19, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.970\tacc: 0.853\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.888\tacc: 0.954\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.929\tacc: 0.904\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 20, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.012\tacc: 0.881\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.926\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.969\tacc: 0.921\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 21, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.987\tacc: 0.903\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.902\tacc: 0.957\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.944\tacc: 0.930\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 22, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.935\tacc: 0.921\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.887\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.911\tacc: 0.935\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 23, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.998\tacc: 0.921\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.925\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.961\tacc: 0.931\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 24, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.953\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.839\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.896\tacc: 0.943\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 25, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.999\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.870\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.935\tacc: 0.956\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 26, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.920\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.890\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.905\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 27, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.946\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.863\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.905\tacc: 0.955\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 28, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.950\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.856\tacc: 0.941\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.903\tacc: 0.943\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 29, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.939\tacc: 0.958\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.920\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.929\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 30, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.939\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.852\tacc: 0.954\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.896\tacc: 0.957\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 31, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.002\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.784\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.893\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 32, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.922\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.835\tacc: 0.972\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.878\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 33, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 6.004\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.844\tacc: 0.972\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.924\tacc: 0.969\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 34, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.892\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.808\tacc: 0.964\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.850\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 35, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.926\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.809\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.868\tacc: 0.963\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 36, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.880\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.853\tacc: 0.963\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.867\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 37, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.825\tacc: 0.963\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.864\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.845\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 38, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.946\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.814\tacc: 0.973\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.880\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 39, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.919\tacc: 0.964\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.798\tacc: 0.972\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.858\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 40, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.883\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.785\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.834\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 41, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.880\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.809\tacc: 0.958\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.845\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 42, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.832\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.812\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.822\tacc: 0.957\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 43, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.801\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.790\tacc: 0.950\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.795\tacc: 0.959\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 44, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.896\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.780\tacc: 0.956\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.838\tacc: 0.963\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 45, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.811\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.774\tacc: 0.969\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.793\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 46, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.874\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.805\tacc: 0.972\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.840\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 47, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.859\tacc: 0.973\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.789\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.824\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 48, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.843\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.742\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.792\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 49, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.834\tacc: 0.965\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.748\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.791\tacc: 0.963\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 50, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.839\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.764\tacc: 0.956\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.801\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:epochs: 51, training task: 1\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 0\tloss: 5.814\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: 1\tloss: 5.778\tacc: 0.956\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:\ttask: avg\tloss: 5.796\tacc: 0.962\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=560813)\u001b[0m INFO:root:final components: 2\n",
      "INFO:root:No. gpus per agent: 0.5\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:Agent: node_id: 0, seed: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:Agent: node_id: 1, seed: 1000\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:Agent: node_id: 2, seed: 2000\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:Agent: node_id: 3, seed: 3000\n",
      "INFO:root:Created fleet with 4 agents\n",
      "INFO:root:Adding neighbors...\n",
      "INFO:root:Fleet initialized\n"
     ]
    }
   ],
   "source": [
    "fake_dataset = get_dataset(**process_dataset_cfg(cfg))\n",
    "fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                     LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                     train_kwargs=train_cfg, fake_dataset=fake_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init tasks: [3 9 2 4 9 2 5 4 8 9 9 4 1 6 9 6 8 0 5 1]\n",
      "agent tasks\n",
      "[array([4, 7, 0, 8, 4, 6, 4, 3, 8, 0, 1, 0, 5, 9, 6, 4, 9, 0, 3, 0]),\n",
      " array([2, 1, 3, 8, 6, 0, 3, 0, 5, 0, 3, 6, 2, 7, 6, 7, 6, 1, 0, 5]),\n",
      " array([0, 9, 7, 2, 1, 8, 6, 1, 6, 4, 5, 7, 8, 0, 2, 3, 0, 3, 9, 6]),\n",
      " array([0, 5, 3, 4, 4, 5, 9, 3, 0, 6, 9, 1, 3, 1, 7, 9, 5, 2, 3, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(\"init tasks:\", fake_dataset.class_sequence)\n",
    "print(\"agent tasks\")\n",
    "pprint([data.class_sequence for data in datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After initialization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'components.0.weight': 0.0,\n",
       " 'components.0.bias': 0.0,\n",
       " 'components.1.weight': 0.0,\n",
       " 'components.1.bias': 0.0,\n",
       " 'random_linear_projection.weight': 0.0,\n",
       " 'random_linear_projection.bias': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After initialization:\")\n",
    "diff_models(ray.get(fleet.agents[0].get_net.remote()).state_dict(), \n",
    "            ray.get(fleet.agents[1].get_net.remote()).state_dict(),\n",
    "            keys=[\"random_linear_projection\", \"components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy just after initialization:\n",
      "{0: 0.4900497512437811}\n",
      "{0: 0.5237655745269958}\n",
      "{0: 0.4927099044746104}\n",
      "{0: 0.4754273504273504}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy just after initialization:\")\n",
    "# evaluate the performance again\n",
    "for agent in fleet.agents:\n",
    "    print(ray.get(agent.eval.remote(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.188\tacc: 0.490\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.188\tacc: 0.490\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.237\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.237\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.145\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.145\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 0, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.331\tacc: 0.475\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.331\tacc: 0.475\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 1, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.207\tacc: 0.606\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.207\tacc: 0.606\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 1, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.190\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.190\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 1, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.211\tacc: 0.520\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.211\tacc: 0.520\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 1, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.219\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.219\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 2, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.205\tacc: 0.533\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.205\tacc: 0.533\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 2, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.192\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.192\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 2, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.210\tacc: 0.679\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.210\tacc: 0.679\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 2, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.217\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.217\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 3, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 3, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.184\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.184\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 3, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.208\tacc: 0.804\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.208\tacc: 0.804\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 3, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.214\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.214\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 4, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 4, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.177\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.177\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 4, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.205\tacc: 0.796\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.205\tacc: 0.796\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 4, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.213\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.213\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 5, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.201\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.201\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 5, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.178\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.178\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 5, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.202\tacc: 0.791\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.202\tacc: 0.791\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 5, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.207\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.207\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 6, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 6, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.174\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.174\tacc: 0.493\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 7, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.205\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.205\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 6, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.202\tacc: 0.745\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.202\tacc: 0.745\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 6, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.204\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.204\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 7, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.162\tacc: 0.516\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.162\tacc: 0.516\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 8, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.204\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.204\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 7, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.200\tacc: 0.671\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.200\tacc: 0.671\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 7, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 8, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.630\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.630\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 9, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.198\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.198\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 8, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.197\tacc: 0.621\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.197\tacc: 0.621\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 8, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.194\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.194\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 9, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.154\tacc: 0.740\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.154\tacc: 0.740\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 10, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 9, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.596\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.596\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 9, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 10, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.138\tacc: 0.811\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.138\tacc: 0.811\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 11, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.200\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 11, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.125\tacc: 0.869\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.125\tacc: 0.869\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 10, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.189\tacc: 0.588\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.189\tacc: 0.588\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 10, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.186\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.186\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 12, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 12, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.130\tacc: 0.900\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.130\tacc: 0.900\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 13, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.196\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.196\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 11, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.184\tacc: 0.569\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.184\tacc: 0.569\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 11, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.184\tacc: 0.525\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.184\tacc: 0.525\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 13, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.131\tacc: 0.937\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.131\tacc: 0.937\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 14, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 12, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.178\tacc: 0.584\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.178\tacc: 0.584\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 12, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.185\tacc: 0.533\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.185\tacc: 0.533\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 14, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.103\tacc: 0.959\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.103\tacc: 0.959\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 15, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 13, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.186\tacc: 0.563\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.186\tacc: 0.563\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 13, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.186\tacc: 0.598\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.186\tacc: 0.598\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 15, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.121\tacc: 0.969\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.121\tacc: 0.969\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 16, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.203\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 14, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.172\tacc: 0.592\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.172\tacc: 0.592\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 14, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.176\tacc: 0.610\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.176\tacc: 0.610\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 16, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.125\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.125\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 17, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 15, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.176\tacc: 0.602\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.176\tacc: 0.602\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 15, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.177\tacc: 0.666\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.177\tacc: 0.666\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 17, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.113\tacc: 0.974\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.113\tacc: 0.974\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 18, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.201\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.201\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 16, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.179\tacc: 0.609\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.179\tacc: 0.609\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 16, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.179\tacc: 0.709\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.179\tacc: 0.709\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 18, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.106\tacc: 0.975\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.106\tacc: 0.975\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 19, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.199\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 17, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.172\tacc: 0.627\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.172\tacc: 0.627\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 17, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.180\tacc: 0.760\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.180\tacc: 0.760\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 19, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.103\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.103\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 20, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.196\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.196\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 18, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.178\tacc: 0.642\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.178\tacc: 0.642\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 18, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.173\tacc: 0.806\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.173\tacc: 0.806\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 20, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.125\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.125\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 21, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 22, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.194\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.194\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 19, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.181\tacc: 0.672\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.181\tacc: 0.672\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 19, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.182\tacc: 0.838\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.182\tacc: 0.838\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 21, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.112\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.112\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 23, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.202\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 20, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.170\tacc: 0.704\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.170\tacc: 0.704\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 20, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.177\tacc: 0.850\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.177\tacc: 0.850\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 22, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.099\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.099\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 24, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.192\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.192\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 23, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.098\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.098\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 21, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.171\tacc: 0.730\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.171\tacc: 0.730\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 21, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.174\tacc: 0.868\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.174\tacc: 0.868\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 25, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.197\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.197\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 24, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.111\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.111\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 22, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.164\tacc: 0.770\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.164\tacc: 0.770\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 22, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.168\tacc: 0.877\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.168\tacc: 0.877\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 26, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 25, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.083\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.083\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 23, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.163\tacc: 0.817\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.163\tacc: 0.817\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 23, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.170\tacc: 0.892\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.170\tacc: 0.892\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 27, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 26, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.104\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.104\tacc: 0.977\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 24, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.169\tacc: 0.846\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.169\tacc: 0.846\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 24, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.168\tacc: 0.894\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.168\tacc: 0.894\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 28, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 27, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.083\tacc: 0.974\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.083\tacc: 0.974\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 25, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.174\tacc: 0.870\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.174\tacc: 0.870\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 25, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.169\tacc: 0.902\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.169\tacc: 0.902\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 29, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.195\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 28, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.081\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.081\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 26, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.160\tacc: 0.882\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.160\tacc: 0.882\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 30, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.188\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.188\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 26, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.165\tacc: 0.911\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.165\tacc: 0.911\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 29, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.091\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.091\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 27, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.161\tacc: 0.898\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.161\tacc: 0.898\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 31, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.193\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 27, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.167\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.167\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 30, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.074\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.074\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 32, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.192\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.192\tacc: 0.524\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 28, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.170\tacc: 0.917\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.170\tacc: 0.917\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 28, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.165\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.165\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 31, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.080\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.080\tacc: 0.970\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 33, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.191\tacc: 0.525\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.191\tacc: 0.525\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 29, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.161\tacc: 0.922\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.161\tacc: 0.922\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 29, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 32, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.073\tacc: 0.976\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.073\tacc: 0.976\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 34, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.191\tacc: 0.526\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.191\tacc: 0.526\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 30, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.164\tacc: 0.933\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.164\tacc: 0.933\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 30, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.156\tacc: 0.916\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.156\tacc: 0.916\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 33, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.089\tacc: 0.976\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.089\tacc: 0.976\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 35, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.187\tacc: 0.527\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.187\tacc: 0.527\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 31, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.166\tacc: 0.936\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.166\tacc: 0.936\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 31, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.165\tacc: 0.912\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.165\tacc: 0.912\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 34, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.067\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.067\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 36, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.191\tacc: 0.534\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.191\tacc: 0.534\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 32, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.161\tacc: 0.936\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.161\tacc: 0.936\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 32, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.160\tacc: 0.912\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.160\tacc: 0.912\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 35, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.074\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.074\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 37, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.187\tacc: 0.551\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.187\tacc: 0.551\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 33, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.941\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.941\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 33, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.166\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.166\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 36, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.077\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.077\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 38, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.185\tacc: 0.580\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.185\tacc: 0.580\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 34, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 34, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.156\tacc: 0.920\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.156\tacc: 0.920\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 37, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.074\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.074\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 39, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.190\tacc: 0.628\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.190\tacc: 0.628\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 35, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.946\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.946\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 35, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.162\tacc: 0.926\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.162\tacc: 0.926\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 38, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.050\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.050\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 40, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.190\tacc: 0.650\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.190\tacc: 0.650\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 41, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.184\tacc: 0.658\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.184\tacc: 0.658\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 36, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.145\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.145\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 36, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.155\tacc: 0.938\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.155\tacc: 0.938\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 39, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.059\tacc: 0.981\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.059\tacc: 0.981\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 42, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.183\tacc: 0.677\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.183\tacc: 0.677\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 40, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.069\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.069\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 37, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.161\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.161\tacc: 0.945\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 37, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.158\tacc: 0.941\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.158\tacc: 0.941\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 43, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.183\tacc: 0.695\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.183\tacc: 0.695\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 41, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.064\tacc: 0.981\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.064\tacc: 0.981\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 38, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.151\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.151\tacc: 0.949\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 38, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.151\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.151\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 44, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.183\tacc: 0.703\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.183\tacc: 0.703\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 42, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.053\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.053\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 39, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.157\tacc: 0.952\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.157\tacc: 0.952\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 39, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.151\tacc: 0.927\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.151\tacc: 0.927\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 45, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.181\tacc: 0.717\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.181\tacc: 0.717\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 43, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.048\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.048\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 40, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.147\tacc: 0.955\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.147\tacc: 0.955\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 40, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.152\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.152\tacc: 0.915\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 46, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.180\tacc: 0.748\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.180\tacc: 0.748\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 44, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.043\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.043\tacc: 0.978\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 41, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.156\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.156\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 41, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.153\tacc: 0.911\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.153\tacc: 0.911\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 47, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.177\tacc: 0.772\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.177\tacc: 0.772\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 45, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.043\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.043\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 42, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.153\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.153\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 42, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.147\tacc: 0.917\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.147\tacc: 0.917\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 48, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.176\tacc: 0.793\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.176\tacc: 0.793\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 46, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.044\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.044\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 43, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.151\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.151\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 49, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.185\tacc: 0.804\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.185\tacc: 0.804\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 43, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.146\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.146\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 47, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.038\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.038\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 50, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.175\tacc: 0.750\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.175\tacc: 0.750\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 44, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.153\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.153\tacc: 0.968\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 44, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.148\tacc: 0.930\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.148\tacc: 0.930\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:W/update: 0.76, WO/update: 0.52\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:Keeping new module. Total: 3\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 48, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.050\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.050\tacc: 0.980\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:epochs: 51, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: 0\tloss: 6.175\tacc: 0.750\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:\ttask: avg\tloss: 6.175\tacc: 0.750\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620693)\u001b[0m INFO:root:final components: 3\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 45, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.161\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.161\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 45, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.143\tacc: 0.935\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.143\tacc: 0.935\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 49, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.060\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.060\tacc: 0.979\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 46, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.153\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.153\tacc: 0.967\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 46, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.144\tacc: 0.938\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.144\tacc: 0.938\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 50, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.011\tacc: 0.982\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.011\tacc: 0.982\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 47, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.148\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.148\tacc: 0.966\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:W/update: 0.97, WO/update: 0.96\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:Not keeping new module. Total: 2\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 47, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.147\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.147\tacc: 0.940\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:epochs: 51, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: 0\tloss: 6.011\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:\ttask: avg\tloss: 6.011\tacc: 0.971\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620634)\u001b[0m INFO:root:final components: 2\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 48, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.149\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.149\tacc: 0.961\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 48, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.141\tacc: 0.942\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.141\tacc: 0.942\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 49, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.146\tacc: 0.958\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.146\tacc: 0.958\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 49, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.146\tacc: 0.947\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.146\tacc: 0.947\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 50, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.117\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.117\tacc: 0.960\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 50, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.113\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.113\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:W/update: 0.95, WO/update: 0.93\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:Not keeping new module. Total: 2\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:W/update: 0.96, WO/update: 0.79\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:Keeping new module. Total: 3\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:epochs: 51, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: 0\tloss: 6.123\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:\ttask: avg\tloss: 6.123\tacc: 0.924\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620538)\u001b[0m INFO:root:final components: 2\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:epochs: 51, training task: 0\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: 0\tloss: 6.124\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:\ttask: avg\tloss: 6.124\tacc: 0.951\n",
      "\u001b[2m\u001b[36m(ParallelModGrad pid=620586)\u001b[0m INFO:root:final components: 3\n"
     ]
    }
   ],
   "source": [
    "# local train\n",
    "fleet.train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After local training:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'components.0.weight': 0.0028489402029663324,\n",
       " 'components.0.bias': 0.0028158421628177166,\n",
       " 'components.1.weight': 0.0027418900281190872,\n",
       " 'components.1.bias': 0.003049996914342046,\n",
       " 'random_linear_projection.weight': 0.0,\n",
       " 'random_linear_projection.bias': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After local training:\")\n",
    "\n",
    "diff_models(ray.get(fleet.agents[0].get_net.remote()).state_dict(), \n",
    "            ray.get(fleet.agents[1].get_net.remote()).state_dict(),\n",
    "            keys=[\"random_linear_projection\", \"components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after local train:\n",
      "{0: 0.9238805970149254}\n",
      "{0: 0.9510844485463775}\n",
      "{0: 0.971342383107089}\n",
      "{0: 0.75}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy after local train:\")\n",
    "# evaluate the performance again\n",
    "for agent in fleet.agents:\n",
    "    print(ray.get(agent.eval.remote(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# check the number of components\n",
    "for agent in fleet.agents:\n",
    "    print(len(ray.get(agent.get_net.remote()).components))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: check that structures, decoders, encoders and non-shared modules are NOT changed because they are not being\n",
    "aggregated and also not being finetuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1_local = ray.get(fleet.agents[0].get_net.remote()) # net1 after local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet.num_coms_per_round = 10\n",
    "fleet.communicate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after sync:\n",
      "{0: 0.927860696517413}\n",
      "{0: 0.9478541762805722}\n",
      "{0: 0.9703368526897939}\n",
      "{0: 0.8637820512820513}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy after sync:\")\n",
    "# evaluate the performance again\n",
    "for agent in fleet.agents:\n",
    "    print(ray.get(agent.eval.remote(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sync between agents:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'components.0.weight': 0.0019303187727928162,\n",
       " 'components.0.bias': 0.0022637085057795048,\n",
       " 'components.1.weight': 0.0018625843804329634,\n",
       " 'components.1.bias': 0.0022147209383547306,\n",
       " 'random_linear_projection.weight': 0.0,\n",
       " 'random_linear_projection.bias': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After sync between agents:\")\n",
    "\n",
    "diff_models(ray.get(fleet.agents[0].get_net.remote()).state_dict(), \n",
    "            ray.get(fleet.agents[1].get_net.remote()).state_dict(),\n",
    "            keys=[\"random_linear_projection\", \"components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sync between agent1 (local) and agent1 (sync):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'structure.0': 0.0,\n",
       " 'structure.1': 0.0,\n",
       " 'structure.2': 0.0,\n",
       " 'structure.3': 0.0,\n",
       " 'structure.4': 0.0,\n",
       " 'structure.5': 0.0,\n",
       " 'structure.6': 0.0,\n",
       " 'structure.7': 0.0,\n",
       " 'structure.8': 0.0,\n",
       " 'structure.9': 0.0,\n",
       " 'components.0.weight': 0.0056788865476846695,\n",
       " 'components.0.bias': 0.005201797932386398,\n",
       " 'components.1.weight': 0.005264018662273884,\n",
       " 'components.1.bias': 0.00524795800447464,\n",
       " 'random_linear_projection.weight': 0.0,\n",
       " 'random_linear_projection.bias': 0.0,\n",
       " 'decoder.0.weight': 0.019980832934379578,\n",
       " 'decoder.0.bias': 0.011504922062158585,\n",
       " 'decoder.1.weight': 0.0,\n",
       " 'decoder.1.bias': 0.0,\n",
       " 'decoder.2.weight': 0.0,\n",
       " 'decoder.2.bias': 0.0,\n",
       " 'decoder.3.weight': 0.0,\n",
       " 'decoder.3.bias': 0.0,\n",
       " 'decoder.4.weight': 0.0,\n",
       " 'decoder.4.bias': 0.0,\n",
       " 'decoder.5.weight': 0.0,\n",
       " 'decoder.5.bias': 0.0,\n",
       " 'decoder.6.weight': 0.0,\n",
       " 'decoder.6.bias': 0.0,\n",
       " 'decoder.7.weight': 0.0,\n",
       " 'decoder.7.bias': 0.0,\n",
       " 'decoder.8.weight': 0.0,\n",
       " 'decoder.8.bias': 0.0,\n",
       " 'decoder.9.weight': 0.0,\n",
       " 'decoder.9.bias': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-03-29 01:12:52,282 E 555291 555291] (raylet) node_manager.cc:3040: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d05aa95ca8792bd0128332767b10b7bec22cda1c8aa3fef916ab42b8, IP: 158.130.50.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 158.130.50.18`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-03-29 15:05:53,064 E 555291 555291] (raylet) node_manager.cc:3040: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d05aa95ca8792bd0128332767b10b7bec22cda1c8aa3fef916ab42b8, IP: 158.130.50.18) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 158.130.50.18`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "print(\"After sync between agent1 (local) and agent1 (sync):\")\n",
    "diff_models(ray.get(fleet.agents[0].get_net.remote()).state_dict(), \n",
    "            net1_local.state_dict(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
