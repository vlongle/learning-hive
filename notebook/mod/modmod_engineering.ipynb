{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "from shell.utils.experiment_utils import *\n",
    "from shell.fleet.utils.fleet_utils import *\n",
    "from shell.utils.metric import *\n",
    "import matplotlib.pyplot as plt\n",
    "from shell.fleet.network import TopologyGenerator\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shell.fleet.fleet import Agent, Fleet\n",
    "from shell.fleet.data.data_utilize import *\n",
    "from shell.fleet.data.recv import *\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.utils import make_grid\n",
    "from shell.fleet.data.data_utilize import *\n",
    "import logging\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "from shell.fleet.data.recv_utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from functools import partial\n",
    "from torchvision.utils import make_grid\n",
    "from shell.utils.oodloss import OODSeparationLoss\n",
    "from pythresh.thresholds.dsn import DSN\n",
    "from pythresh.thresholds.aucp import AUCP\n",
    "from pythresh.thresholds.boot import BOOT\n",
    "from pythresh.thresholds.zscore import ZSCORE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Class sequence: [5 0 4 3 4 6 4 3 8 0 1 0 5 9 6 4 9 0 3 0]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'component_update_freq': 100, 'num_epochs': 100, 'init_component_update_freq': 100, 'init_num_epochs': 100, 'save_freq': 20}, 'dataset': {'dataset_name': 'mnist', 'num_tasks': 10, 'num_classes_per_task': 2, 'with_replacement': True, 'num_trains_per_class': 64, 'num_vals_per_class': 50, 'remap_labels': True}, 'net': {'name': 'mlp', 'depth': 4, 'layer_size': 64, 'dropout': 0.0}, 'sharing_strategy': {'name': 'no_sharing', 'num_coms_per_round': 0}, 'seed': 0, 'algo': 'modular', 'job_name': 'mnist_modular_numtrain_64_contrastive', 'num_agents': 8, 'root_save_dir': 'experiment_results/vanilla_fix_bug_compute_loss_encodev2', 'parallel': True, 'num_init_tasks': 4, 'agent': {'save_dir': '${root_save_dir}/${job_name}/${dataset.dataset_name}/${algo}/seed_${seed}', 'batch_size': 64, 'memory_size': 32, 'improvement_threshold': 0.05, 'use_contrastive': True}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [4 7 5 7 6 0 3 0 5 0 3 6 2 7 6 7 6 1 0 5]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [6 7 7 8 4 1 1 8 6 1 6 4 5 7 8 0 2 3 0 3]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [2 8 0 3 7 4 3 4 4 5 9 3 0 6 9 1 3 1 7 9]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [0 3 0 2 9 7 0 9 2 1 7 6 8 6 1 8 6 4 9 8]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [8 4 6 3 3 1 1 6 4 9 3 2 2 9 6 0 5 9 7 2]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [7 5 8 4 6 9 8 3 4 6 1 3 3 1 4 1 9 2 6 2]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n",
      "INFO:root:Class sequence: [5 1 8 3 9 6 5 9 5 0 7 2 7 8 6 1 6 0 0 6]\n",
      "INFO:root:task 0 :(128, 1, 28, 28)\n",
      "INFO:root:task 1 :(128, 1, 28, 28)\n",
      "INFO:root:task 2 :(128, 1, 28, 28)\n",
      "INFO:root:task 3 :(128, 1, 28, 28)\n",
      "INFO:root:task 4 :(128, 1, 28, 28)\n",
      "INFO:root:task 5 :(128, 1, 28, 28)\n",
      "INFO:root:task 6 :(128, 1, 28, 28)\n",
      "INFO:root:task 7 :(128, 1, 28, 28)\n",
      "INFO:root:task 8 :(128, 1, 28, 28)\n",
      "INFO:root:task 9 :(128, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_size 28\n",
      "num_classes 2\n",
      "net_cfg {'name': 'mlp', 'depth': 4, 'layer_size': 64, 'dropout': 0.0, 'i_size': 28, 'num_classes': 2, 'num_tasks': 10, 'num_init_tasks': 4, 'use_contrastive': True}\n",
      "<class 'shell.learners.er_dynamic.CompositionalDynamicER'>\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\"\n",
    "algo = \"modular\"\n",
    "\n",
    "experiment_folder = \"experiment_results\"\n",
    "experiment_name = \"vanilla_fix_bug_compute_loss_encodev2\"\n",
    "\n",
    "use_contrastive = True\n",
    "num_trains_per_class = 64\n",
    "seed = 0\n",
    "num_tasks = 10\n",
    "parallel = False\n",
    "comm_freq = 2 # \"None\" means no communication, doesn't matter for this analysis\n",
    "\n",
    "\n",
    "save_dir = get_save_dir(experiment_folder, experiment_name,\n",
    "                        dataset, algo, num_trains_per_class, use_contrastive, seed)\n",
    "graph, datasets, NetCls, LearnerCls, net_cfg, agent_cfg, train_cfg, fleet_additional_cfg, cfg = get_cfg(\n",
    "    save_dir)\n",
    "\n",
    "module_selection_strategy = 'gt_most_similar'\n",
    "\n",
    "cfg.sharing_strategy = DictConfig({\n",
    "    \"name\": \"modmod\",\n",
    "    'num_coms_per_round': 2,\n",
    "    'module_selection': module_selection_strategy,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentCls = get_agent_cls(cfg.sharing_strategy, cfg.algo, parallel)\n",
    "FleetCls = get_fleet(cfg.sharing_strategy, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_fleet(task_id=None): \n",
    "    fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "    if task_id is not None:\n",
    "        fleet.load_model_from_ckpoint(task_ids=task_id)\n",
    "    return fleet\n",
    "\n",
    "def add_random_module(receiver, task_id, check=True):\n",
    "    if check and len(receiver.net.candidate_indices) != 0:\n",
    "        return\n",
    "    receiver.net.add_tmp_modules(task_id, num_modules=1)\n",
    "\n",
    "def transfer_module(receiver, sender, task_id, check=True):\n",
    "    new_module = sender.net.components[-1]\n",
    "    add_random_module(receiver, task_id, check=check)\n",
    "    receiver.net.receive_modules(task_id, [new_module])\n",
    "\n",
    "def transfer_decoder(receiver, sender, rcv_task_id, sender_task_id=None):\n",
    "    if sender_task_id is None:\n",
    "        sender_task_id = rcv_task_id\n",
    "    receiver.net.decoder[rcv_task_id].load_state_dict(sender.net.decoder[sender_task_id].state_dict())\n",
    "\n",
    "@torch.no_grad()\n",
    "def transfer_structure(receiver, sender, task_id):\n",
    "    target = receiver.net.structure[task_id]\n",
    "    source = sender.net.structure[task_id]\n",
    "    if target.shape == source.shape:\n",
    "        target.copy_(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_counterfactual_scenario(scenario, task_id=None, agent_id=None):\n",
    "    total_results = defaultdict(dict)\n",
    "    task_ids = range(cfg.num_init_tasks, num_tasks) if task_id is None else [task_id]\n",
    "    agent_ids = range(len(setup_fleet().agents)) if agent_id is None else [agent_id]\n",
    "\n",
    "    for t_id in task_ids:\n",
    "        print('task', t_id)\n",
    "        target_fleet = setup_fleet(t_id-1)\n",
    "        source_fleet = setup_fleet(t_id)\n",
    "        for a_id in agent_ids:\n",
    "            print('a_id', a_id)\n",
    "            target_agent = target_fleet.agents[a_id]\n",
    "            source_agent = source_fleet.agents[a_id] \n",
    "\n",
    "            results = {'Raw': None, 'Transfer Decoder': None, 'Decoder + Structure': None}\n",
    "\n",
    "            if scenario == \"no new module\":\n",
    "                pass\n",
    "            elif scenario == \"optimized module\":\n",
    "                transfer_module(target_agent, source_agent, t_id)\n",
    "            elif scenario == \"random module\":\n",
    "                add_random_module(target_agent, t_id)\n",
    "\n",
    "            results['Raw'] = target_agent.eval_test(t_id)\n",
    "            \n",
    "            transfer_decoder(target_agent, source_agent, t_id)\n",
    "            results['Transfer Decoder'] = target_agent.eval_test(t_id)\n",
    "\n",
    "            transfer_structure(target_agent, source_agent, t_id)\n",
    "            results['Decoder + Structure'] = target_agent.eval_test(t_id)\n",
    "\n",
    "            total_results[t_id][a_id] = results\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opt_module_struct(scenario, target_agent, source_agent, task_id, train_candidate_module=True):\n",
    "    if \"optimized_module\" in scenario:\n",
    "        transfer_module(target_agent, source_agent, task_id)\n",
    "        train_candidate_module = False if \"frozen\" in scenario else True\n",
    "    elif scenario == \"random_module\":\n",
    "        add_random_module(target_agent, task_id)\n",
    "        train_candidate_module = True\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    target_agent.agent.T = task_id\n",
    "    target_agent.update_replay_buffer(task_id-1)\n",
    "    target_agent.agent.change_save_dir(target_agent.agent.save_dir.replace(experiment_name, experiment_name + \"_modmod_eng_\" + scenario))\n",
    "    # opt the last component\n",
    "    # target_agent.net.active_candidate_index = target_agent.net.num_components-1\n",
    "    # target_agent.net.candidate_indices = [target_agent.net.num_components-1]\n",
    "    # print(target_agent.net.candidate_indices, target_agent.net.active_candidate_index)\n",
    "\n",
    "    target_agent.train(task_id, train_candidate_module=train_candidate_module,\n",
    "                       save_freq=10,\n",
    "                       num_candidate_modules=0,)\n",
    "    return target_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_scenario(scenario, task_id, agent_id):\n",
    "    target_fleet = setup_fleet(task_id-1)\n",
    "    target_agent = target_fleet.agents[agent_id]\n",
    "\n",
    "    source_fleet = setup_fleet(task_id)\n",
    "    source_agent = source_fleet.agents[agent_id] \n",
    "    return run_opt_module_struct(scenario, target_agent, source_agent, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_speed_scenario(scenario, source_agent_id, target_agent_id,\n",
    "                                source_task, target_task):\n",
    "    fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                        LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                        train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "    source_agent = fleet.agents[source_agent_id]\n",
    "    target_agent = fleet.agents[target_agent_id]\n",
    "\n",
    "    source_agent.load_model_from_ckpoint(task_id=source_task)\n",
    "    target_agent.load_model_from_ckpoint(task_id=target_task-1)\n",
    "    return run_opt_module_struct(scenario, target_agent, source_agent, target_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual\n",
    "Same exact model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_performance(res):\n",
    "    categories = ['Raw', 'Transfer Decoder', 'Decoder + Structure']\n",
    "    task_ids = sorted(res.keys())\n",
    "    means = {category: [] for category in categories}\n",
    "    stds = {category: [] for category in categories}\n",
    "\n",
    "    for t_id in task_ids:\n",
    "        for category in categories:\n",
    "            scores = [res[t_id][a_id][category][t_id] for a_id in res[t_id]]\n",
    "            means[category].append(np.mean(scores))\n",
    "            stds[category].append(np.std(scores))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for category in categories:\n",
    "        mean_scores = means[category]\n",
    "        std_scores = stds[category]\n",
    "        plt.plot(task_ids, mean_scores, label=category)\n",
    "        plt.fill_between(task_ids, np.subtract(mean_scores, std_scores), np.add(mean_scores, std_scores), alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Average Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_counterfactual_scenario(\"no new module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_counterfactual_scenario(\"optimized module\")\n",
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_counterfactual_scenario(\"random module\")\n",
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning speed\n",
    "Only transfer the module. Receiver optimized the structure and decoder on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent = run_training_scenario(\"optimized_module_frozen\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_training_scenario(\"random_module\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen_target_agent = run_training_scenario(\"optimized_module\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test_task with epoch\n",
    "test_task = 4\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Similarity between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Agent: node_id: 0, seed: 0\n",
      "INFO:root:Agent: node_id: 1, seed: 1000\n",
      "INFO:root:Agent: node_id: 2, seed: 2000\n",
      "INFO:root:Agent: node_id: 3, seed: 3000\n",
      "INFO:root:Agent: node_id: 4, seed: 4000\n",
      "INFO:root:Agent: node_id: 5, seed: 5000\n",
      "INFO:root:Agent: node_id: 6, seed: 6000\n",
      "INFO:root:Agent: node_id: 7, seed: 7000\n",
      "INFO:root:Created fleet with 8 agents\n",
      "INFO:root:Adding neighbors...\n",
      "INFO:root:Fleet initialized\n"
     ]
    }
   ],
   "source": [
    "fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 0, 2\n",
    "source_task, target_task = 4, 7\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet.load_records()\n",
    "fleet.load_model_from_ckpoint(task_ids=target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet.communicate_round(target_task, end_epoch=-1, comm_freq=-1, num_epochs=-1,\n",
    "                        communication_round=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 3 4 6 4 3 8 0 1 0 5 9 6 4]\n",
      "[4 7 5 7 6 0 3 0 5 0 3 6 2 7 6 7]\n",
      "[6 7 7 8 4 1 1 8 6 1 6 4 5 7 8 0]\n",
      "[2 8 0 3 7 4 3 4 4 5 9 3 0 6 9 1]\n",
      "[0 3 0 2 9 7 0 9 2 1 7 6 8 6 1 8]\n",
      "[8 4 6 3 3 1 1 6 4 9 3 2 2 9 6 0]\n",
      "[7 5 8 4 6 9 8 3 4 6 1 3 3 1 4 1]\n",
      "[5 1 8 3 9 6 5 9 5 0 7 2 7 8 6 1]\n"
     ]
    }
   ],
   "source": [
    "for agent in fleet.agents:\n",
    "    print(agent.dataset.class_sequence[:(target_task + 1)* agent.dataset.num_classes_per_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet.communicate_round(target_task, end_epoch=-1, comm_freq=-1, num_epochs=-1,\n",
    "                        communication_round=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(4, 1.0, Linear(in_features=64, out_features=64, bias=True))],\n",
       " 1: [(4,\n",
       "   0.3333333333333333,\n",
       "   Linear(in_features=64, out_features=64, bias=True))],\n",
       " 3: [(6,\n",
       "   0.3333333333333333,\n",
       "   Linear(in_features=64, out_features=64, bias=True))],\n",
       " 4: [(7,\n",
       "   0.3333333333333333,\n",
       "   Linear(in_features=64, out_features=64, bias=True))],\n",
       " 5: [(7,\n",
       "   0.3333333333333333,\n",
       "   Linear(in_features=64, out_features=64, bias=True))],\n",
       " 6: [],\n",
       " 7: [(4,\n",
       "   0.3333333333333333,\n",
       "   Linear(in_features=64, out_features=64, bias=True))]}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_agent.incoming_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'component_update_freq': 100,\n",
       " 'num_epochs': 100,\n",
       " 'init_component_update_freq': 100,\n",
       " 'init_num_epochs': 100,\n",
       " 'save_freq': 20,\n",
       " 'module_list': (4, 1.0, Linear(in_features=64, out_features=64, bias=True))}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "target_agent.train_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_modules_record = os.path.join(\n",
    "    source_agent.save_dir, \"add_modules_record.csv\")\n",
    "df = pd.read_csv(add_modules_record)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_modules_record = os.path.join(\n",
    "    target_agent.save_dir, \"add_modules_record.csv\")\n",
    "df = pd.read_csv(add_modules_record)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent.load_model_from_ckpoint(task_id=source_task)\n",
    "target_agent.load_model_from_ckpoint(task_id=target_task-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.eval_test(target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_decoder(target_agent, source_agent, target_task, source_task)\n",
    "# target_agent.eval_test(target_task)\n",
    "'''\n",
    "{0: 0.9615384615384616,\n",
    " 1: 0.9839357429718876,\n",
    " 2: 0.9695876288659794,\n",
    " 3: 0.9809236947791165,\n",
    " 4: 0.4984646878198567,\n",
    " 'avg': 0.8788900431950604}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_module(target_agent, source_agent, target_task)\n",
    "# target_agent.eval_test(target_task)\n",
    "'''\n",
    "{0: 0.9615384615384616,\n",
    " 1: 0.9839357429718876,\n",
    " 2: 0.9695876288659794,\n",
    " 3: 0.9809236947791165,\n",
    " 4: 0.4984646878198567,\n",
    " 'avg': 0.8788900431950604}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two tasks have something in common but not completely identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 1, 0\n",
    "source_task, target_task = 4, 4\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df\n",
    "\n",
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source and target tasks have nothing in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 2, 0\n",
    "source_task, target_task = 4, 4\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df\n",
    "\n",
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition between two modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_task_sim(cls_seq1, cls_seq2, task_id1, task_id2, \n",
    "                     num_classes_per_task=2):\n",
    "    task_cls1 = cls_seq1[task_id1 * num_classes_per_task: (task_id1 + 1) * num_classes_per_task]\n",
    "    task_cls2 = cls_seq2[task_id2 * num_classes_per_task: (task_id2 + 1) * num_classes_per_task]\n",
    "    return len(set(task_cls1).intersection(set(task_cls2))), task_cls1, task_cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "\n",
    "source_agent_id1, source_agent_id2, target_agent_id = 2, 1, 0\n",
    "source_task1, source_task2, target_task = 7, 4, 4\n",
    "\n",
    "source_agent1 = fleet.agents[source_agent_id1]\n",
    "source_agent2 = fleet.agents[source_agent_id2]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "\n",
    "print(compute_task_sim(source_agent1.dataset.class_sequence, target_agent.dataset.class_sequence, source_task1, target_task))\n",
    "print(compute_task_sim(source_agent2.dataset.class_sequence, target_agent.dataset.class_sequence, source_task2, target_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent1.load_model_from_ckpoint(task_id=source_task1)\n",
    "source_agent2.load_model_from_ckpoint(task_id=source_task2)\n",
    "target_agent.load_model_from_ckpoint(task_id=target_task-1)\n",
    "\n",
    "# transfer_module(target_agent, source_agent1, target_task, check=False)\n",
    "# transfer_module(target_agent, source_agent2, target_task, check=False)\n",
    "\n",
    "module1 = source_agent1.net.components[-1]\n",
    "module2 = source_agent2.net.components[-1]\n",
    "\n",
    "target_agent.net.add_tmp_modules(target_task, num_modules=2)\n",
    "target_agent.net.receive_modules(target_task, [module1, module2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_agent.net.components))\n",
    "print(target_agent.net.candidate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.eval_test(target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.agent.T = target_task\n",
    "target_agent.update_replay_buffer(target_task-1)\n",
    "target_agent.agent.change_save_dir(target_agent.agent.save_dir.replace(experiment_name, experiment_name + \"_modmod_eng_\" + \"two_mods\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.train(target_task, train_candidate_module=True,\n",
    "                       save_freq=10,\n",
    "                       num_candidate_modules=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id1, target_agent_id,\n",
    "                                source_task1, target_task)\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = target_agent.agent.record.df\n",
    "test_task = target_task\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')                                                                                             \n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
