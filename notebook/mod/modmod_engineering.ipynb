{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "from shell.utils.experiment_utils import *\n",
    "from shell.fleet.utils.fleet_utils import *\n",
    "from shell.utils.metric import *\n",
    "import matplotlib.pyplot as plt\n",
    "from shell.fleet.network import TopologyGenerator\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shell.fleet.fleet import Agent, Fleet\n",
    "from shell.fleet.data.data_utilize import *\n",
    "from shell.fleet.data.recv import *\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.utils import make_grid\n",
    "from shell.fleet.data.data_utilize import *\n",
    "import logging\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "from shell.fleet.data.recv_utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from functools import partial\n",
    "from torchvision.utils import make_grid\n",
    "from shell.utils.oodloss import OODSeparationLoss\n",
    "from pythresh.thresholds.dsn import DSN\n",
    "from pythresh.thresholds.aucp import AUCP\n",
    "from pythresh.thresholds.boot import BOOT\n",
    "from pythresh.thresholds.zscore import ZSCORE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "algo = \"modular\"\n",
    "prefilter_strategy = \"None\"\n",
    "scorer = \"cross_entropy\"\n",
    "\n",
    "experiment_folder = \"experiment_results\"\n",
    "experiment_name = \"vanilla_fix_bug_compute_loss_encodev2\"\n",
    "\n",
    "use_contrastive = True\n",
    "num_trains_per_class = 64\n",
    "seed = 0\n",
    "num_tasks = 10\n",
    "parallel = False\n",
    "comm_freq = None  # \"None\" means no communication, doesn't matter for this analysis\n",
    "\n",
    "\n",
    "save_dir = get_save_dir(experiment_folder, experiment_name,\n",
    "                        dataset, algo, num_trains_per_class, use_contrastive, seed)\n",
    "graph, datasets, NetCls, LearnerCls, net_cfg, agent_cfg, train_cfg, fleet_additional_cfg, cfg = get_cfg(\n",
    "    save_dir)\n",
    "\n",
    "cfg.sharing_strategy = DictConfig({\n",
    "    \"name\": \"recv_data\",\n",
    "    \"scorer\": scorer,\n",
    "    \"num_queries\": 5,\n",
    "    'num_data_neighbors': 5,\n",
    "    'num_filter_neighbors': 5,\n",
    "    'num_coms_per_round': 2,\n",
    "    \"query_score_threshold\": 0.0,\n",
    "    \"shared_memory_size\": 50,\n",
    "    \"comm_freq\": comm_freq,\n",
    "    \"prefilter_strategy\": prefilter_strategy,\n",
    "    \"use_ood_separation_loss\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentCls = get_agent_cls(cfg.sharing_strategy, cfg.algo, parallel)\n",
    "FleetCls = get_fleet(cfg.sharing_strategy, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_fleet(task_id=None): \n",
    "    fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "    if task_id is not None:\n",
    "        fleet.load_model_from_ckpoint(task_ids=task_id)\n",
    "    return fleet\n",
    "\n",
    "def add_random_module(receiver, task_id, check=True):\n",
    "    if check and len(receiver.net.candidate_indices) != 0:\n",
    "        return\n",
    "    receiver.net.add_tmp_modules(task_id, num_modules=1)\n",
    "\n",
    "def transfer_module(receiver, sender, task_id, check=True):\n",
    "    new_module = sender.net.components[-1]\n",
    "    add_random_module(receiver, task_id, check=check)\n",
    "    receiver.net.receive_modules(task_id, [new_module])\n",
    "\n",
    "def transfer_decoder(receiver, sender, rcv_task_id, sender_task_id=None):\n",
    "    if sender_task_id is None:\n",
    "        sender_task_id = rcv_task_id\n",
    "    receiver.net.decoder[rcv_task_id].load_state_dict(sender.net.decoder[sender_task_id].state_dict())\n",
    "\n",
    "@torch.no_grad()\n",
    "def transfer_structure(receiver, sender, task_id):\n",
    "    target = receiver.net.structure[task_id]\n",
    "    source = sender.net.structure[task_id]\n",
    "    if target.shape == source.shape:\n",
    "        target.copy_(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_counterfactual_scenario(scenario, task_id=None, agent_id=None):\n",
    "    total_results = defaultdict(dict)\n",
    "    task_ids = range(cfg.num_init_tasks, num_tasks) if task_id is None else [task_id]\n",
    "    agent_ids = range(len(setup_fleet().agents)) if agent_id is None else [agent_id]\n",
    "\n",
    "    for t_id in task_ids:\n",
    "        print('task', t_id)\n",
    "        target_fleet = setup_fleet(t_id-1)\n",
    "        source_fleet = setup_fleet(t_id)\n",
    "        for a_id in agent_ids:\n",
    "            print('a_id', a_id)\n",
    "            target_agent = target_fleet.agents[a_id]\n",
    "            source_agent = source_fleet.agents[a_id] \n",
    "\n",
    "            results = {'Raw': None, 'Transfer Decoder': None, 'Decoder + Structure': None}\n",
    "\n",
    "            if scenario == \"no new module\":\n",
    "                pass\n",
    "            elif scenario == \"optimized module\":\n",
    "                transfer_module(target_agent, source_agent, t_id)\n",
    "            elif scenario == \"random module\":\n",
    "                add_random_module(target_agent, t_id)\n",
    "\n",
    "            results['Raw'] = target_agent.eval_test(t_id)\n",
    "            \n",
    "            transfer_decoder(target_agent, source_agent, t_id)\n",
    "            results['Transfer Decoder'] = target_agent.eval_test(t_id)\n",
    "\n",
    "            transfer_structure(target_agent, source_agent, t_id)\n",
    "            results['Decoder + Structure'] = target_agent.eval_test(t_id)\n",
    "\n",
    "            total_results[t_id][a_id] = results\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opt_module_struct(scenario, target_agent, source_agent, task_id, train_candidate_module=True):\n",
    "    if \"optimized_module\" in scenario:\n",
    "        transfer_module(target_agent, source_agent, task_id)\n",
    "        train_candidate_module = False if \"frozen\" in scenario else True\n",
    "    elif scenario == \"random_module\":\n",
    "        add_random_module(target_agent, task_id)\n",
    "        train_candidate_module = True\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    target_agent.agent.T = task_id\n",
    "    target_agent.update_replay_buffer(task_id-1)\n",
    "    target_agent.agent.change_save_dir(target_agent.agent.save_dir.replace(experiment_name, experiment_name + \"_modmod_eng_\" + scenario))\n",
    "    # opt the last component\n",
    "    # target_agent.net.active_candidate_index = target_agent.net.num_components-1\n",
    "    # target_agent.net.candidate_indices = [target_agent.net.num_components-1]\n",
    "    # print(target_agent.net.candidate_indices, target_agent.net.active_candidate_index)\n",
    "\n",
    "    target_agent.train(task_id, train_candidate_module=train_candidate_module,\n",
    "                       save_freq=10,\n",
    "                       num_candidate_modules=0,)\n",
    "    return target_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_scenario(scenario, task_id, agent_id):\n",
    "    target_fleet = setup_fleet(task_id-1)\n",
    "    target_agent = target_fleet.agents[agent_id]\n",
    "\n",
    "    source_fleet = setup_fleet(task_id)\n",
    "    source_agent = source_fleet.agents[agent_id] \n",
    "    return run_opt_module_struct(scenario, target_agent, source_agent, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_speed_scenario(scenario, source_agent_id, target_agent_id,\n",
    "                                source_task, target_task):\n",
    "    fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                        LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                        train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "    source_agent = fleet.agents[source_agent_id]\n",
    "    target_agent = fleet.agents[target_agent_id]\n",
    "\n",
    "    source_agent.load_model_from_ckpoint(task_id=source_task)\n",
    "    target_agent.load_model_from_ckpoint(task_id=target_task-1)\n",
    "    return run_opt_module_struct(scenario, target_agent, source_agent, target_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual\n",
    "Same exact model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_performance(res):\n",
    "    categories = ['Raw', 'Transfer Decoder', 'Decoder + Structure']\n",
    "    task_ids = sorted(res.keys())\n",
    "    means = {category: [] for category in categories}\n",
    "    stds = {category: [] for category in categories}\n",
    "\n",
    "    for t_id in task_ids:\n",
    "        for category in categories:\n",
    "            scores = [res[t_id][a_id][category][t_id] for a_id in res[t_id]]\n",
    "            means[category].append(np.mean(scores))\n",
    "            stds[category].append(np.std(scores))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for category in categories:\n",
    "        mean_scores = means[category]\n",
    "        std_scores = stds[category]\n",
    "        plt.plot(task_ids, mean_scores, label=category)\n",
    "        plt.fill_between(task_ids, np.subtract(mean_scores, std_scores), np.add(mean_scores, std_scores), alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Average Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_id 3\n"
     ]
    }
   ],
   "source": [
    "res = run_counterfactual_scenario(\"no new module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_counterfactual_scenario(\"optimized module\")\n",
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_counterfactual_scenario(\"random module\")\n",
    "plot_performance(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning speed\n",
    "Only transfer the module. Receiver optimized the structure and decoder on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent = run_training_scenario(\"optimized_module_frozen\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_training_scenario(\"random_module\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen_target_agent = run_training_scenario(\"optimized_module\", task_id=4, agent_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test_task with epoch\n",
    "test_task = 4\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Similarity between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in fleet.agents:\n",
    "    print(agent.dataset.class_sequence[2*4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 2, 0\n",
    "source_task, target_task = 7, 4\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_modules_record = os.path.join(\n",
    "    source_agent.save_dir, \"add_modules_record.csv\")\n",
    "df = pd.read_csv(add_modules_record)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_modules_record = os.path.join(\n",
    "    target_agent.save_dir, \"add_modules_record.csv\")\n",
    "df = pd.read_csv(add_modules_record)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent.load_model_from_ckpoint(task_id=source_task)\n",
    "target_agent.load_model_from_ckpoint(task_id=target_task-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.eval_test(target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_decoder(target_agent, source_agent, target_task, source_task)\n",
    "# target_agent.eval_test(target_task)\n",
    "'''\n",
    "{0: 0.9615384615384616,\n",
    " 1: 0.9839357429718876,\n",
    " 2: 0.9695876288659794,\n",
    " 3: 0.9809236947791165,\n",
    " 4: 0.4984646878198567,\n",
    " 'avg': 0.8788900431950604}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_module(target_agent, source_agent, target_task)\n",
    "# target_agent.eval_test(target_task)\n",
    "'''\n",
    "{0: 0.9615384615384616,\n",
    " 1: 0.9839357429718876,\n",
    " 2: 0.9695876288659794,\n",
    " 3: 0.9809236947791165,\n",
    " 4: 0.4984646878198567,\n",
    " 'avg': 0.8788900431950604}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two tasks have something in common but not completely identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 1, 0\n",
    "source_task, target_task = 4, 4\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df\n",
    "\n",
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source and target tasks have nothing in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent_id, target_agent_id = 2, 0\n",
    "source_task, target_task = 4, 4\n",
    "source_agent = fleet.agents[source_agent_id]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "# make sure that they have the same tasks\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(make_grid(source_agent.dataset.trainset[source_task].tensors[0]).permute(1, 2, 0))\n",
    "ax[1].imshow(make_grid(target_agent.dataset.trainset[target_task].tensors[0]).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "unfrozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "frozen_mod_target_agent = run_learning_speed_scenario(\"optimized_module_frozen\", source_agent_id, target_agent_id,\n",
    "                                source_task, target_task)\n",
    "df = frozen_mod_target_agent.agent.record.df\n",
    "unfrozen_df = unfrozen_mod_target_agent.agent.record.df\n",
    "random_module_df = random_module_target_agent.agent.record.df\n",
    "\n",
    "# plot test_task with epoch\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "unfrozen_df[unfrozen_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                               label=\"Unfrozen\",\n",
    "                                                                                                               marker='o')\n",
    "\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition between two modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_task_sim(cls_seq1, cls_seq2, task_id1, task_id2, \n",
    "                     num_classes_per_task=2):\n",
    "    task_cls1 = cls_seq1[task_id1 * num_classes_per_task: (task_id1 + 1) * num_classes_per_task]\n",
    "    task_cls2 = cls_seq2[task_id2 * num_classes_per_task: (task_id2 + 1) * num_classes_per_task]\n",
    "    return len(set(task_cls1).intersection(set(task_cls2))), task_cls1, task_cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet = FleetCls(graph, cfg.seed, datasets, cfg.sharing_strategy, AgentCls, NetCls=NetCls,\n",
    "                    LearnerCls=LearnerCls, net_kwargs=net_cfg, agent_kwargs=agent_cfg,\n",
    "                    train_kwargs=train_cfg, **fleet_additional_cfg)\n",
    "\n",
    "source_agent_id1, source_agent_id2, target_agent_id = 2, 1, 0\n",
    "source_task1, source_task2, target_task = 7, 4, 4\n",
    "\n",
    "source_agent1 = fleet.agents[source_agent_id1]\n",
    "source_agent2 = fleet.agents[source_agent_id2]\n",
    "target_agent = fleet.agents[target_agent_id]\n",
    "\n",
    "print(compute_task_sim(source_agent1.dataset.class_sequence, target_agent.dataset.class_sequence, source_task1, target_task))\n",
    "print(compute_task_sim(source_agent2.dataset.class_sequence, target_agent.dataset.class_sequence, source_task2, target_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_agent1.load_model_from_ckpoint(task_id=source_task1)\n",
    "source_agent2.load_model_from_ckpoint(task_id=source_task2)\n",
    "target_agent.load_model_from_ckpoint(task_id=target_task-1)\n",
    "\n",
    "# transfer_module(target_agent, source_agent1, target_task, check=False)\n",
    "# transfer_module(target_agent, source_agent2, target_task, check=False)\n",
    "\n",
    "module1 = source_agent1.net.components[-1]\n",
    "module2 = source_agent2.net.components[-1]\n",
    "\n",
    "target_agent.net.add_tmp_modules(target_task, num_modules=2)\n",
    "target_agent.net.receive_modules(target_task, [module1, module2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(target_agent.net.components))\n",
    "print(target_agent.net.candidate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.eval_test(target_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.agent.T = target_task\n",
    "target_agent.update_replay_buffer(target_task-1)\n",
    "target_agent.agent.change_save_dir(target_agent.agent.save_dir.replace(experiment_name, experiment_name + \"_modmod_eng_\" + \"two_mods\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_agent.train(target_task, train_candidate_module=True,\n",
    "                       save_freq=10,\n",
    "                       num_candidate_modules=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_module_target_agent = run_learning_speed_scenario(\"random_module\", source_agent_id1, target_agent_id,\n",
    "                                source_task1, target_task)\n",
    "random_module_df = random_module_target_agent.agent.record.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = target_agent.agent.record.df\n",
    "test_task = target_task\n",
    "fig, ax = plt.subplots()\n",
    "df[df['test_task'] == test_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax, label=\"Frozen\",\n",
    "                                                                                             marker='o')\n",
    "random_module_df[random_module_df['test_task'] == target_task].groupby(['epoch']).mean(numeric_only=True)['test_acc'].plot(ax=ax,\n",
    "                                                                                                                label=\"Random\",\n",
    "                                                                                                                marker='o')                                                                                             \n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.2, 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
