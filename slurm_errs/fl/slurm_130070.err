/mnt/kostas-graid/sw/envs/vlongle/miniconda3/envs/shell/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
2023-10-17 21:22:35,466	INFO worker.py:1544 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:Agent: node_id: 69420, seed: 69420000
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 0
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 0
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:final components: 4
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 1
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69558991	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69459139	acc: 0.493
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 1
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69558991	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69459139	acc: 0.493
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:final components: 4
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 2
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69558991	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69335657	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69417978	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 2
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69558991	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69335657	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69417978	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:final components: 4
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 0, training task: 3
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69359287	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69558991	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69335657	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 3	loss: 0.69281258	acc: 0.500
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69383798	acc: 0.499
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 1, training task: 3
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69473484	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69548005	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69424823	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 3	loss: 0.69246993	acc: 0.500
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69423326	acc: 0.499
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 2, training task: 3
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69647945	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69499845	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69606043	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 3	loss: 0.69091098	acc: 0.500
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69461233	acc: 0.499
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:epochs: 3, training task: 3
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 0	loss: 0.69876529	acc: 0.488
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 1	loss: 0.69378819	acc: 0.498
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 2	loss: 0.69919441	acc: 0.509
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: 3	loss: 0.68737078	acc: 0.500
[2m[36m(ParallelModGrad pid=940773)[0m INFO:root:	task: avg	loss: 0.69477967	acc: 0.499
